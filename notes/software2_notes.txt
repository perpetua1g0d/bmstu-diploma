Kubernetes - оркестратор контейнеров, позволяет управлять множеством контейнеров на разных серверах централизованно.
Преимущества (реклама):
immutable [containers, nodes] (артефакт можно запустить на ноуте, сервер, ...)
declarative (декларативные конфиги)
self-healing (сервер может сам поднять упавший сервер)
decoupling [app, cluster] - каждый компонент независимый (один компонент упал - остальные продолжают работать, а не падают по эффекту домино)

API - центральная точка кубера, является REST

кубер - не совсем оркестратор, он полноценная кластерная-операционная система (lect2, 36:00)

OAuth Token Exchange (RFC 8693)

kube spray [installer]?

pod - минимальная абстракция (!) k8s, одно запущенное приложение (1 инстанс). 
Внутри пода есть контейнеры, как минимум 2 - приложение и POD_... [несет в себе network namespaces].
Внутри пода по дефолту свои volume'ы у контейнеров, но можно делать и общие (можно даже у разных подов сделать общие)
нода - условный сервер, который будет запускаться в рамках запуска приложения (?)

прометей файл конфига можно подмонитровать средствами кубера. прометей не умеет отслеживать изменения в своем конфиге.
- можно взять еще один компонент component-reloader(?) - отдельный контейнер, живет в поде вместе с прометеем

ports:
	- containerPort: 80 # просто документация, кубер это игнорирует

ReplicaSet - под с указанном количеством экземпляров. Cоздатся N подов с именами <my-pod>-afds4, <my-pod>-opsd4, ...
По лейблу легко листить поды - k get pod -l ...

k scale --replicas 3 replicaset <> - скейлит до указанного количества подов

kubctl get pod - выводить все поды в неймспейсе
k describe po my-pod - выводит всю инфу про под, например IP, containers, mounts, conditions, events ...


postgres, redis, ... - PaaS? or DBaaS?

containers: resources (requests, limits: cpu, memory)



+-------------------+          +-------------------+
|  Service A (Pod)  |          |  Service B (Pod)  |
|-------------------|          |-------------------|
| PostgreSQL        |          | PostgreSQL        |
| Sidecar (Golang)  | ---HTTP--> Sidecar (Golang)  |
+-------------------+          +-------------------+
       |                                |
       +-------- Kubernetes DNS -------+
       
       
       
4. Анализ использования Istio
Плюсы:

Автоматическое взаимное TLS

Централизованное управление политиками

Мониторинг трафика

Минусы для вашего случая:

Требует >1GB памяти на под

Сложность настройки для SQL-запросов

Избыточность для локальной демонстрации

Рекомендация:
Используйте сайдкар-авторизацию без Istio. Это:

Сохранит ресурсы

Позволит детально продемонстрировать логику авторизации

Будет работать на слабом железе

requests,time_ms,operation,sign_enabled,sign_enabled
100,1.23,write,true,true
250,0.95,write,true,true
500,1.09,write,true,true
750,1.31,write,true,true
1000,1.52,write,true,true

requests,time_ms,operation,sign_enabled,sign_enabled
100,1.11,write,false,false
250,0.98,write,false,false
500,1.07,write,false,false
750,1.25,write,false,false
1000,1.42,write,false,false


kubectl get --raw /openid/v1/jwks
{"keys":[{"use":"sig","kty":"RSA","kid":"ScTeo5TofAdww284ofdrtbzLdsT___4pNhp7uOQcYB8","alg":"RS256","n":"xo-9nsCMRWG9opN3Z75SW9M1AbqW8P1G5hwfUNo7LTSTyPC6GpwvLJGB-SVsNZqBy_kVZS-1lPLZp_l2_36ekpmP_MuW3iue6eIXjdQoYBo7oZSk2pdI0svAEG2eqSW2jp5o-a9B5U0L2E0AHWlUiylrv0p_Lj6r0-dBGIaKCItQcafnIbDp4vZ4k2Q2FYHqf6IyCN-_Hb9hwt1uMPGkahnMq5tMWZfAv_HxwIzyXFm3lxzsAru_UHFYpCilskR8viopCU4M2cedP4fyAvq56OlVtOs1yxNC4W9fHfEeklZqB6lhOGU4KpEy1Y467qHwc5izzjkJtFQxfPyWVzihkQ","e":"AQAB"}]}


Почитать что гпт пишет, это вполне можно занести в технологический раздел. 

Но в требованиях указано: "пользователь сможет менять значения ... на сайдкаре заданного сервисе, например, postgres-a или postgres-b". Поэтому реализуем выбор сервиса.

Для этого:

- В ConfigMap `auth-settings` должен быть в каждом неймспейсе (postgres-a, postgres-b и т.д.).

- UI должен иметь права на чтение и запись ConfigMap в разных неймспейсах.

- В форме UI добавим выпадающий список неймспейсов.

Изменим RoleBinding на ClusterRoleBinding, чтобы дать доступ к ConfigMap в любом неймспейсе, или создадим RoleBinding для каждого неймспейса. Второй вариант сложнее, поэтому выберем ClusterRole с ограничением по ресурсам.

Обновим RBAC:

- Создадим ClusterRole, которая разрешает доступ к ConfigMap с именем `auth-settings` в любом неймспейсе.

- Привяжем эту ClusterRole к ServiceAccount UI.

Однако, это даст доступ ко всем ConfigMap с именем `auth-settings` в кластере, что безопасно, потому что имя ConfigMap фиксировано.

Вместо этого можно создать Role для каждого неймспейса, но это потребует создания множества RoleBinding. Для простоты сделаем ClusterRole.

Но учтем, что в разных неймспейсах могут быть ConfigMap с одинаковым именем `auth-settings` — это нормально.


- Для сбора метрик из подов с аннотациями, Prometheus Operator использует ServiceMonitors или PodMonitors. В нашем случае, мы используем аннотации, поэтому в Prometheus должен быть включен автоматический discovery подов с аннотациями.



